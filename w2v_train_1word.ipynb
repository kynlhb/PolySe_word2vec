{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Read data\n",
    "filename = 'orginal_corpus.txt'\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read().split('\\n')\n",
    "    return data\n",
    "\n",
    "all_words = read_data(filename)\n",
    "vocabulary = all_words[0].split()\n",
    "print('Data size', len(vocabulary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 5000000\n",
    "\n",
    "\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "#     count = [['UNK', -1]]\n",
    "    count = []\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "#     data = list()\n",
    "#     unk_count = 0\n",
    "#     for word in words:\n",
    "#         index = dictionary.get(word, 0)\n",
    "#         if index == 0:  # dictionary['UNK']\n",
    "#             unk_count += 1\n",
    "#         data.append(index)\n",
    "#     count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return count, dictionary, reversed_dictionary\n",
    "\n",
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                            vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the sentences that include the target word\n",
    "\n",
    "def tag_word(words, target_word, skip_window):\n",
    "#     i = 1\n",
    "    count = 0\n",
    "    new_vocabulary = list()\n",
    "\n",
    "    for word in words:\n",
    "        if word == target_word:\n",
    "#             word = word + str(i)\n",
    "#             i+=1\n",
    "            f.write(' '.join(words[count-skip_window:count]))\n",
    "            f.write(' ')\n",
    "            f.write(' '.join(words[count+1:count+1+skip_window]))\n",
    "            f.write(' ')\n",
    "        new_vocabulary.append(word)\n",
    "        count+=1\n",
    "    return new_vocabulary\n",
    "\n",
    "with open('sentence_woman.txt', 'w') as f:\n",
    "    new_vocabulary = tag_word(vocabulary, 'woman', 5)\n",
    "    \n",
    "del new_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644), ('is', 183153), ('as', 131815), ('eight', 125285), ('for', 118445), ('s', 116710), ('five', 115789), ('three', 114775), ('was', 112807), ('by', 111831), ('that', 109510), ('four', 108182), ('six', 102145), ('seven', 99683), ('with', 95603), ('on', 91250), ('are', 76527), ('it', 73334), ('from', 72871), ('or', 68945), ('his', 62603), ('an', 61925), ('be', 61281), ('this', 58832), ('which', 54788), ('at', 54576), ('he', 53573), ('also', 44358), ('not', 44033), ('have', 39712), ('were', 39086), ('has', 37866), ('but', 35358), ('other', 32433), ('their', 31523), ('its', 29567), ('first', 28810), ('they', 28553), ('some', 28161), ('had', 28100), ('all', 26229), ('more', 26223), ('most', 25563), ('can', 25519), ('been', 25383), ('such', 24413), ('many', 24096), ('who', 23997), ('new', 23770), ('used', 22737), ('there', 22707), ('after', 21125), ('when', 20623), ('into', 20484), ('american', 20477), ('time', 20412), ('these', 19864), ('only', 19463), ('see', 19206), ('may', 19115), ('than', 18807), ('world', 17949), ('i', 17581), ('b', 17516), ('would', 17377), ('d', 17236), ('no', 16155), ('however', 15861), ('between', 15737), ('about', 15574), ('over', 15122), ('years', 14935), ('states', 14916), ('people', 14696), ('war', 14629), ('during', 14578), ('united', 14494), ('known', 14437), ('if', 14420), ('called', 14151), ('use', 14011), ('th', 13380), ('system', 13296), ('often', 12987), ('state', 12904), ('so', 12722), ('history', 12623), ('will', 12560), ('up', 12445), ('while', 12363), ('where', 12347), ('city', 12275), ('being', 11931), ('english', 11868), ('then', 11847), ('any', 11803), ('both', 11755), ('under', 11753), ('out', 11721), ('made', 11701), ('well', 11537), ('her', 11536), ('e', 11426), ('number', 11399), ('government', 11323), ('them', 11285), ('m', 10976), ('later', 10971), ('since', 10691), ('him', 10629), ('part', 10627), ('name', 10572), ('c', 10561), ('century', 10550), ('through', 10371), ('because', 10332), ('x', 10307), ('university', 10195), ('early', 10172), ('life', 10096), ('british', 10056), ('year', 9858), ('like', 9854), ('same', 9774), ('including', 9633), ('became', 9591), ('example', 9539), ('day', 9534), ('each', 9500), ('even', 9412), ('work', 9388), ('language', 9375), ('although', 9286), ('several', 9168), ('form', 9133), ('john', 8956), ('u', 8928), ('national', 8904), ('very', 8861), ('much', 8822), ('g', 8773), ('french', 8736), ('before', 8700), ('general', 8659), ('what', 8581), ('t', 8491), ('against', 8432), ('n', 8372), ('high', 8337), ('links', 8312), ('could', 8304), ('based', 8244), ('those', 8209), ('now', 8206), ('second', 8110), ('de', 8002), ('music', 7987), ('another', 7933), ('large', 7898), ('she', 7896), ('f', 7878), ('external', 7862), ('german', 7858), ('different', 7797), ('modern', 7790), ('great', 7770), ('do', 7763), ('common', 7698), ('set', 7682), ('list', 7672), ('south', 7628), ('series', 7611), ('major', 7585), ('game', 7553), ('power', 7522), ('long', 7488), ('country', 7481), ('king', 7456), ('law', 7435), ('group', 7417), ('film', 7400), ('still', 7378), ('until', 7368), ('north', 7328), ('international', 7262), ('term', 7219), ('we', 7118), ('end', 7113), ('book', 7110), ('found', 7043), ('own', 7034), ('political', 6970), ('party', 6943), ('order', 6908), ('usually', 6872), ('president', 6865), ('church', 6786), ('you', 6690), ('death', 6684), ('theory', 6604), ('area', 6591), ('around', 6576), ('include', 6531), ('god', 6518), ('ii', 6494), ('way', 6433), ('did', 6419), ('military', 6410), ('population', 6400), ('using', 6384), ('though', 6362), ('small', 6361), ('following', 6300), ('within', 6238), ('non', 6220), ('left', 6184), ('human', 6184), ('main', 6166), ('among', 6146), ('point', 6141), ('r', 6064), ('due', 6056), ('p', 6001), ('considered', 5985), ('public', 5976), ('popular', 5967), ('computer', 5874), ('west', 5862), ('family', 5857), ('east', 5854), ('information', 5844), ('important', 5843), ('european', 5806), ('man', 5778), ('sometimes', 5761), ('right', 5758), ('old', 5711), ('free', 5684), ('word', 5678), ('without', 5661), ('last', 5654), ('us', 5652), ('members', 5634), ('given', 5605), ('times', 5582), ('roman', 5468), ('make', 5450), ('h', 5404), ('age', 5350), ('place', 5345), ('l', 5343), ('thus', 5319), ('science', 5314), ('case', 5289), ('become', 5268), ('systems', 5262), ('union', 5248), ('born', 5246), ('york', 5243), ('line', 5236), ('countries', 5231), ('does', 5221), ('isbn', 5216), ('st', 5207), ('control', 5196), ('various', 5163), ('others', 5160), ('house', 5157), ('article', 5143), ('island', 5124), ('should', 5113), ('led', 5108), ('back', 5105), ('period', 5099), ('player', 5096), ('europe', 5094), ('languages', 5087), ('central', 5070), ('water', 5025), ('few', 5013), ('western', 5010), ('home', 5007), ('began', 5004), ('generally', 4979), ('less', 4974), ('k', 4970), ('similar', 4939), ('written', 4916), ('original', 4910), ('best', 4902), ('must', 4898), ('according', 4884), ('school', 4872), ('france', 4813), ('air', 4802), ('single', 4801), ('force', 4776), ('v', 4762), ('land', 4755), ('groups', 4731), ('down', 4728), ('how', 4727), ('works', 4724), ('development', 4721), ('official', 4720), ('support', 4686), ('england', 4641), ('j', 4612), ('rather', 4605), ('data', 4586), ('space', 4586), ('greek', 4577), ('km', 4574), ('named', 4551), ('germany', 4539), ('just', 4528), ('games', 4527), ('said', 4483), ('version', 4472), ('late', 4471), ('earth', 4457), ('company', 4448), ('every', 4446), ('economic', 4435), ('short', 4433), ('published', 4430), ('black', 4429), ('army', 4418), ('off', 4414), ('london', 4399), ('million', 4386), ('body', 4383), ('field', 4352), ('christian', 4347), ('either', 4330), ('social', 4307), ('empire', 4307), ('o', 4299), ('developed', 4270), ('standard', 4269), ('court', 4268), ('service', 4260), ('kingdom', 4257), ('along', 4241), ('college', 4236), ('republic', 4231), ('sea', 4212), ('america', 4202), ('today', 4198), ('result', 4193), ('held', 4178), ('team', 4176), ('light', 4172), ('means', 4165), ('never', 4127), ('especially', 4125), ('third', 4121), ('further', 4118), ('forces', 4114), ('character', 4114), ('take', 4105), ('men', 4078), ('society', 4067), ('show', 4064), ('open', 4063), ('possible', 4051), ('fact', 4042), ('battle', 4034), ('took', 4013), ('former', 4007), ('books', 3992), ('soviet', 3985), ('river', 3984), ('children', 3978), ('having', 3964), ('good', 3962), ('local', 3960), ('current', 3955), ('son', 3955), ('process', 3949), ('natural', 3947), ('present', 3922), ('himself', 3919), ('islands', 3903), ('total', 3891), ('near', 3874), ('white', 3866), ('days', 3863), ('person', 3855), ('itself', 3843), ('seen', 3831), ('culture', 3809), ('little', 3805), ('above', 3782), ('software', 3773), ('largest', 3772), ('words', 3771), ('upon', 3768), ('level', 3767), ('father', 3762), ('created', 3760), ('side', 3760), ('red', 3755), ('references', 3747), ('press', 3745), ('full', 3734), ('region', 3726), ('almost', 3717), ('al', 3714), ('image', 3714), ('famous', 3708), ('play', 3702), ('came', 3698), ('role', 3691), ('once', 3686), ('certain', 3675), ('league', 3663), ('jewish', 3662), ('james', 3653), ('january', 3650), ('site', 3639), ('again', 3616), ('numbers', 3611), ('art', 3611), ('member', 3601), ('areas', 3599), ('movement', 3595), ('religious', 3588), ('type', 3588), ('march', 3583), ('community', 3581), ('story', 3573), ('played', 3572), ('production', 3570), ('released', 3555), ('center', 3548), ('rights', 3546), ('real', 3545), ('related', 3536), ('foreign', 3528), ('low', 3524), ('ancient', 3523), ('terms', 3519), ('view', 3519), ('source', 3503), ('act', 3502), ('minister', 3493), ('change', 3483), ('energy', 3464), ('produced', 3457), ('research', 3452), ('actor', 3451), ('making', 3448), ('december', 3443), ('civil', 3443), ('women', 3442), ('special', 3441), ('style', 3438), ('japanese', 3437), ('design', 3437), ('william', 3437), ('available', 3435), ('chinese', 3430), ('forms', 3429), ('canada', 3428), ('northern', 3423), ('died', 3418), ('class', 3412), ('living', 3410), ('next', 3406), ('particular', 3404), ('program', 3403), ('council', 3401), ('television', 3395), ('head', 3376), ('david', 3368), ('china', 3365), ('middle', 3363), ('established', 3360), ('hand', 3356), ('bc', 3356), ('far', 3352), ('july', 3333), ('function', 3330), ('position', 3318), ('y', 3311), ('built', 3310), ('george', 3307), ('band', 3304), ('together', 3303), ('w', 3301)]\n"
     ]
    }
   ],
   "source": [
    "# del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:500])\n",
    "# print('Sample data', [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = read_data(\"sentence_woman.txt\")\n",
    "train_data = train_data[0].split()\n",
    "trained_word = 'woman'\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(train_data, trained_word):\n",
    "    all_input = np.ndarray(shape=(len(train_data)), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(len(train_data), 1), dtype=np.int32)\n",
    "    all_input.fill(dictionary[trained_word])\n",
    "    for i, word in enumerate(train_data):\n",
    "        labels[i, 0] = dictionary[word]\n",
    "    return all_input, labels\n",
    "\n",
    "# batch, labels = generate_batch(train_data, trained_word, 8)\n",
    "# for i in range(8):\n",
    "#     print(batch[i], reverse_dictionary[batch[i]],\n",
    "#         '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 100\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 5       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64      # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 2     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_tests = np.random.choice(valid_window, valid_size, replace=False)\n",
    "valid_examples = np.append(valid_tests, dictionary['woman'])\n",
    "print(valid_examples[2])\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([len(dictionary), embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # Explanation of the meaning of NCE loss:\n",
    "    #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  67926.5405884\n",
      "Average loss at step  200 :  209.290919073\n",
      "Average loss at step  400 :  147.076715871\n",
      "Average loss at step  600 :  122.053367651\n",
      "Average loss at step  800 :  104.809137571\n",
      "Average loss at step  1000 :  92.8199263941\n",
      "Average loss at step  1200 :  83.2380847703\n",
      "Average loss at step  1400 :  75.4570656467\n",
      "Average loss at step  1600 :  68.7821472524\n",
      "Average loss at step  1800 :  63.2852490649\n",
      "Average loss at step  2000 :  58.6979804171\n",
      "Average loss at step  2200 :  54.6283764818\n",
      "Average loss at step  2400 :  50.8451898168\n",
      "Average loss at step  2600 :  47.4702152351\n",
      "Average loss at step  2800 :  44.1339972874\n",
      "Average loss at step  3000 :  41.6467426927\n",
      "Average loss at step  3200 :  39.241179752\n",
      "Average loss at step  3400 :  37.0988287364\n",
      "Average loss at step  3600 :  35.0281831554\n",
      "Average loss at step  3800 :  33.1837953808\n",
      "Average loss at step  4000 :  31.6068613662\n",
      "Average loss at step  4200 :  29.8547594323\n",
      "Average loss at step  4400 :  28.3571664633\n",
      "Average loss at step  4600 :  26.8715332503\n",
      "Average loss at step  4800 :  25.8228721194\n",
      "Average loss at step  5000 :  24.7007588443\n",
      "Average loss at step  5200 :  23.6019951013\n",
      "Average loss at step  5400 :  22.5189462827\n",
      "Average loss at step  5600 :  21.4814251728\n",
      "Average loss at step  5800 :  20.7442797509\n",
      "Average loss at step  6000 :  19.9174846357\n",
      "Average loss at step  6200 :  19.0807848733\n",
      "Average loss at step  6400 :  18.5096393717\n",
      "Average loss at step  6600 :  17.7421658454\n",
      "Average loss at step  6800 :  17.0284168602\n",
      "Average loss at step  7000 :  16.4947917488\n",
      "Average loss at step  7200 :  15.9252900108\n",
      "Average loss at step  7400 :  15.3580034346\n",
      "Average loss at step  7600 :  14.9439228329\n",
      "Average loss at step  7800 :  14.3291213504\n",
      "Average loss at step  8000 :  13.8966174908\n",
      "Average loss at step  8200 :  13.5850245153\n",
      "Average loss at step  8400 :  13.0567973844\n",
      "Average loss at step  8600 :  12.688761132\n",
      "Average loss at step  8800 :  12.3283724384\n",
      "Average loss at step  9000 :  12.0270665737\n",
      "Average loss at step  9200 :  11.8100740475\n",
      "Average loss at step  9400 :  11.3508395431\n",
      "Average loss at step  9600 :  11.1030713333\n",
      "Average loss at step  9800 :  10.7415638166\n",
      "Average loss at step  10000 :  10.5840431297\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    inputs, labels = generate_batch(train_data, trained_word)\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "#         batch_labels = np.zeros((batch_size, n_classes))\n",
    "        batch_labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        for batch_num in xrange(int(len(inputs)/batch_size)+1):\n",
    "            if (batch_num+1)*batch_size <= len(inputs):\n",
    "                batch_inputs = inputs[batch_num*batch_size:(batch_num+1)*batch_size]\n",
    "                batch_labels[:, 0] = labels[batch_num*batch_size:(batch_num+1)*batch_size, 0]\n",
    "#                 batch_labels[range(batch_size), labels[batch_num*batch_size:(batch_num+1)*batch_size, 0]] = 1\n",
    "            else:\n",
    "                batch_inputs = inputs[-batch_size:]\n",
    "                batch_labels[:, 0] = labels[-batch_size:, 0]\n",
    "#                 batch_labels[range(batch_size), labels[-batch_size:, 0]] = 1\n",
    "            feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "#             print(loss_val)\n",
    "            average_loss += loss_val\n",
    "        \n",
    "#         for i in xrange(valid_size+1):\n",
    "#             check_embeddings = valid_embeddings.eval()\n",
    "#             valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#             print(check_embeddings[i])\n",
    "#             print(valid_word)\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= (200*(int(len(inputs)/batch_size)+1))\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "#         # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "#         if step % 10000 == 0:\n",
    "#             sim = similarity.eval()\n",
    "#             for i in xrange(valid_size):\n",
    "#                 valid_word = reverse_dictionary[valid_examples[i]]\n",
    "#                 top_k = 8  # number of nearest neighbors\n",
    "#                 nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "#                 log_str = 'Nearest to %s:' % valid_word\n",
    "#             for k in xrange(top_k):\n",
    "#                 close_word = reverse_dictionary[nearest[k]]\n",
    "#                 log_str = '%s %s,' % (log_str, close_word)\n",
    "#             print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('woman_vector.txt', 'w') as f:\n",
    "    f.write(str(final_embeddings[dictionary['woman']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0357807   0.1857841   0.01946772  0.0205264  -0.01824022  0.01648284\n",
      " -0.10532656  0.10455012 -0.0506693  -0.02614136 -0.06788473  0.067092\n",
      " -0.07097854 -0.01832454  0.17562512 -0.09220994 -0.08865318 -0.01852003\n",
      " -0.1891235  -0.08961862  0.01966646  0.05443218  0.03370234 -0.09657079\n",
      " -0.11356673  0.01233688 -0.00444836 -0.06135167  0.04103355  0.0118425\n",
      "  0.03579991 -0.1657719  -0.00836371  0.0686646  -0.00647543  0.04998835\n",
      " -0.11685289 -0.03666879  0.00310926 -0.0009377   0.08625925 -0.03770928\n",
      " -0.13039523  0.0562083  -0.07692081 -0.08389106 -0.08173437 -0.10759465\n",
      " -0.07719949 -0.09055722  0.16130936  0.04092021 -0.0648022  -0.01815654\n",
      "  0.11379946 -0.0024388  -0.00265408 -0.11260857 -0.0980937   0.00605923\n",
      "  0.05985097  0.07598352  0.06128407 -0.05992256 -0.05869611 -0.02000132\n",
      "  0.08525773 -0.0696595  -0.00175977 -0.01509145  0.0749114  -0.00337901\n",
      "  0.08225035  0.09047309 -0.09357306  0.04469706 -0.01230907 -0.11616122\n",
      "  0.02150053  0.00456217 -0.15452112 -0.04630809  0.12655947 -0.07303987\n",
      "  0.06617898  0.14772603 -0.11121508  0.07390604 -0.02339433 -0.05315552\n",
      " -0.04666528 -0.06606939 -0.06859711  0.11263811 -0.10085724  0.06358577\n",
      "  0.17806621  0.00166338  0.04982362  0.16310927 -0.08740936 -0.07186848\n",
      " -0.0917649   0.06192271 -0.10781368  0.0465068  -0.07779206  0.23061027\n",
      " -0.0259747   0.04149799  0.07898694  0.18938845 -0.00071533  0.01631117\n",
      " -0.09416672  0.0145723  -0.1043752   0.0396325  -0.08427209 -0.12474301\n",
      "  0.18258443  0.08021154 -0.05083812  0.06080748 -0.28371739 -0.06077434\n",
      " -0.12454515  0.03616551]\n"
     ]
    }
   ],
   "source": [
    "print(final_embeddings[dictionary['woman']])\n",
    "woman = final_embeddings[dictionary['woman']]\n",
    "woman = np.array(woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06426143 -0.00837654  0.1980134  -0.09742513 -0.10437928 -0.00150868\n",
      "  0.09878582  0.06168188 -0.07201536 -0.11429717 -0.07504471  0.02948793\n",
      "  0.07345533  0.04259725  0.11041805 -0.00494413  0.07051653 -0.11891647\n",
      " -0.01062964  0.00798341 -0.02633937 -0.07363377  0.15297718  0.1345118\n",
      "  0.03939788  0.12761478 -0.03470821  0.12602098  0.00810388  0.0877671\n",
      "  0.00168014 -0.03119589  0.17897098  0.10663807 -0.06820287  0.14468075\n",
      " -0.020985   -0.00459365 -0.10359303 -0.05125836  0.09031818 -0.13061424\n",
      "  0.1480469  -0.1074915  -0.01883516  0.0162647   0.02106079 -0.00410639\n",
      " -0.11259528 -0.1454927  -0.06305808  0.0483107   0.06163573 -0.08252724\n",
      "  0.12252723 -0.11078162 -0.02730912 -0.0281108  -0.04359945  0.13812022\n",
      " -0.02614157  0.09190316 -0.0052021  -0.04769276  0.09219858  0.07776697\n",
      " -0.04105809 -0.06728956  0.06101124 -0.15189616 -0.04483667 -0.09586494\n",
      " -0.05187369 -0.02582284  0.11297669 -0.04946613 -0.14020644 -0.08537365\n",
      " -0.13278346  0.05018449  0.07249071  0.07443805 -0.14362812  0.15985423\n",
      "  0.02290909 -0.08001291  0.1339239  -0.01899522  0.0529473   0.12080525\n",
      " -0.13830988  0.00823423  0.03806611  0.00463596  0.08813316  0.03474797\n",
      " -0.04699161 -0.02391873 -0.07227785  0.16362631  0.02726053 -0.03693298\n",
      " -0.01087115 -0.06992907 -0.03544131 -0.06004997 -0.07407713 -0.20325746\n",
      "  0.03689325 -0.01923681 -0.13377549 -0.01261471  0.01282861  0.06298161\n",
      "  0.13330215 -0.09261915 -0.03571277 -0.13735898 -0.07306235 -0.05227819\n",
      "  0.15543039  0.12054769 -0.06806238  0.1012755  -0.13383301  0.06115448\n",
      "  0.04878619 -0.10003781]\n",
      "[-0.14401263  0.03133213  0.10876009  0.16138153  0.01993224  0.04053933\n",
      "  0.02407939  0.11829058 -0.09660213  0.05322566  0.15185139 -0.06808496\n",
      " -0.08030491  0.00205176 -0.05553271  0.13937525 -0.04895727 -0.0200414\n",
      " -0.06129832  0.11245506 -0.02979986  0.05640989  0.00764067  0.01651493\n",
      "  0.00163749 -0.06755417 -0.06297332  0.13367759  0.02575959 -0.06324068\n",
      " -0.02061389 -0.12079421 -0.04861513 -0.06412487  0.22672339 -0.14655454\n",
      "  0.04374257  0.08781171 -0.00547297  0.05237079 -0.17510109  0.01255522\n",
      " -0.16211067  0.14800885  0.04263682 -0.01509207 -0.03117405  0.10070733\n",
      "  0.05270626 -0.07679772  0.10136274 -0.03757891  0.08042792 -0.08155028\n",
      " -0.15516576 -0.0857062  -0.05004138 -0.15370208 -0.08123562  0.0412583\n",
      "  0.00415061 -0.09087764  0.08707889 -0.05980315  0.1635002   0.10163325\n",
      "  0.00823487  0.07759993  0.04444237 -0.11905646 -0.04587984  0.02590295\n",
      "  0.02649482  0.07092221 -0.15362558  0.08152816  0.18418971 -0.05635915\n",
      "  0.02716513  0.00310327 -0.16568978  0.00091071 -0.19549216  0.0342464\n",
      "  0.06616123 -0.01854955  0.01579936  0.04912784 -0.04897791 -0.02565535\n",
      "  0.05528686  0.0444776  -0.01629809 -0.08517649  0.04435289 -0.00282378\n",
      " -0.06897477 -0.06155508 -0.11135564 -0.13872746  0.05074596 -0.17721076\n",
      "  0.08092945  0.02020106  0.07460609 -0.05015494 -0.1300696   0.05390926\n",
      "  0.00175319  0.09611303  0.19114007 -0.05178848  0.05972478 -0.12104108\n",
      " -0.03457611 -0.1263997   0.11070186  0.05045658 -0.08912141 -0.03569847\n",
      " -0.11920138  0.02610456 -0.01187746  0.02057748 -0.0062471  -0.02288416\n",
      "  0.03060456 -0.14499602]\n",
      "[ 0.10719869 -0.10232529 -0.02304027 -0.02485728  0.17746936 -0.16133316\n",
      " -0.09617456 -0.08252326  0.03485161 -0.06539463 -0.05119734 -0.0071219\n",
      "  0.04664997  0.07755207 -0.0376812  -0.05126946  0.04974871 -0.09130801\n",
      "  0.02200206 -0.11675026  0.05019299  0.03069795  0.04541247 -0.06323202\n",
      "  0.00760292  0.00975615  0.06263665 -0.15049373  0.10783202 -0.14120691\n",
      "  0.12385137  0.15252414 -0.04066575 -0.01268648 -0.18778183 -0.09001921\n",
      "  0.10144432  0.03953693 -0.05206658  0.08898162 -0.06332143 -0.03476854\n",
      "  0.03007286  0.15931943  0.00775215  0.03130399  0.05636695 -0.04932799\n",
      "  0.00386375 -0.03799838  0.07498509  0.05205622  0.14855604 -0.15902062\n",
      "  0.01181984  0.1059057  -0.05600962  0.00615451 -0.12136336  0.05390648\n",
      "  0.06225348 -0.05576621  0.06548391  0.117924    0.04091459 -0.09727937\n",
      " -0.12719241  0.07945643  0.00403882 -0.08504421  0.01977304  0.01947873\n",
      "  0.02801108  0.04144734  0.06505365  0.04846439 -0.03233812  0.0376178\n",
      " -0.03111132 -0.14116828  0.04352442  0.03442268  0.09748826 -0.10943903\n",
      " -0.02274107 -0.03172036 -0.04356686 -0.02012795  0.14008713 -0.1183772\n",
      " -0.08421071 -0.03655564  0.17045356 -0.02569478 -0.15899356 -0.21644557\n",
      " -0.01376982 -0.06579022 -0.15950891 -0.20833515 -0.13493483 -0.02202897\n",
      " -0.05435896 -0.08009836 -0.05089762 -0.12733521  0.09836871  0.05449677\n",
      "  0.10958663 -0.00039016 -0.05642409 -0.14433326  0.0749191   0.07995714\n",
      "  0.05160326  0.09664808 -0.01128155 -0.05913845  0.02192012 -0.14660437\n",
      "  0.10578209 -0.04522621  0.1281876   0.10627382  0.02222264 -0.15014711\n",
      " -0.03498883  0.03361606]\n",
      "1.00000001524\n"
     ]
    }
   ],
   "source": [
    "king = '0.06426143 -0.00837654  0.1980134  -0.09742513 -0.10437928 -0.00150868\\\n",
    "  0.09878582  0.06168188 -0.07201536 -0.11429717 -0.07504471  0.02948793\\\n",
    "  0.07345533  0.04259725  0.11041805 -0.00494413  0.07051653 -0.11891647\\\n",
    " -0.01062964  0.00798341 -0.02633937 -0.07363377  0.15297718  0.1345118\\\n",
    "  0.03939788  0.12761478 -0.03470821  0.12602098  0.00810388  0.0877671\\\n",
    "  0.00168014 -0.03119589  0.17897098  0.10663807 -0.06820287  0.14468075\\\n",
    " -0.020985   -0.00459365 -0.10359303 -0.05125836  0.09031818 -0.13061424\\\n",
    "  0.1480469  -0.1074915  -0.01883516  0.0162647   0.02106079 -0.00410639\\\n",
    " -0.11259528 -0.1454927  -0.06305808  0.0483107   0.06163573 -0.08252724\\\n",
    "  0.12252723 -0.11078162 -0.02730912 -0.0281108  -0.04359945  0.13812022\\\n",
    " -0.02614157  0.09190316 -0.0052021  -0.04769276  0.09219858  0.07776697\\\n",
    " -0.04105809 -0.06728956  0.06101124 -0.15189616 -0.04483667 -0.09586494\\\n",
    " -0.05187369 -0.02582284  0.11297669 -0.04946613 -0.14020644 -0.08537365\\\n",
    " -0.13278346  0.05018449  0.07249071  0.07443805 -0.14362812  0.15985423\\\n",
    "  0.02290909 -0.08001291  0.1339239  -0.01899522  0.0529473   0.12080525\\\n",
    " -0.13830988  0.00823423  0.03806611  0.00463596  0.08813316  0.03474797\\\n",
    " -0.04699161 -0.02391873 -0.07227785  0.16362631  0.02726053 -0.03693298\\\n",
    " -0.01087115 -0.06992907 -0.03544131 -0.06004997 -0.07407713 -0.20325746\\\n",
    "  0.03689325 -0.01923681 -0.13377549 -0.01261471  0.01282861  0.06298161\\\n",
    "  0.13330215 -0.09261915 -0.03571277 -0.13735898 -0.07306235 -0.05227819\\\n",
    "  0.15543039  0.12054769 -0.06806238  0.1012755  -0.13383301  0.06115448\\\n",
    "  0.04878619 -0.10003781'\n",
    "king = king.split()\n",
    "king = map(float, king)\n",
    "king = np.array(king)\n",
    "print(king)\n",
    "\n",
    "man = '-0.14401263  0.03133213  0.10876009  0.16138153  0.01993224  0.04053933\\\n",
    "  0.02407939  0.11829058 -0.09660213  0.05322566  0.15185139 -0.06808496\\\n",
    " -0.08030491  0.00205176 -0.05553271  0.13937525 -0.04895727 -0.0200414\\\n",
    " -0.06129832  0.11245506 -0.02979986  0.05640989  0.00764067  0.01651493\\\n",
    "  0.00163749 -0.06755417 -0.06297332  0.13367759  0.02575959 -0.06324068\\\n",
    " -0.02061389 -0.12079421 -0.04861513 -0.06412487  0.22672339 -0.14655454\\\n",
    "  0.04374257  0.08781171 -0.00547297  0.05237079 -0.17510109  0.01255522\\\n",
    " -0.16211067  0.14800885  0.04263682 -0.01509207 -0.03117405  0.10070733\\\n",
    "  0.05270626 -0.07679772  0.10136274 -0.03757891  0.08042792 -0.08155028\\\n",
    " -0.15516576 -0.0857062  -0.05004138 -0.15370208 -0.08123562  0.0412583\\\n",
    "  0.00415061 -0.09087764  0.08707889 -0.05980315  0.1635002   0.10163325\\\n",
    "  0.00823487  0.07759993  0.04444237 -0.11905646 -0.04587984  0.02590295\\\n",
    "  0.02649482  0.07092221 -0.15362558  0.08152816  0.18418971 -0.05635915\\\n",
    "  0.02716513  0.00310327 -0.16568978  0.00091071 -0.19549216  0.0342464\\\n",
    "  0.06616123 -0.01854955  0.01579936  0.04912784 -0.04897791 -0.02565535\\\n",
    "  0.05528686  0.0444776  -0.01629809 -0.08517649  0.04435289 -0.00282378\\\n",
    " -0.06897477 -0.06155508 -0.11135564 -0.13872746  0.05074596 -0.17721076\\\n",
    "  0.08092945  0.02020106  0.07460609 -0.05015494 -0.1300696   0.05390926\\\n",
    "  0.00175319  0.09611303  0.19114007 -0.05178848  0.05972478 -0.12104108\\\n",
    " -0.03457611 -0.1263997   0.11070186  0.05045658 -0.08912141 -0.03569847\\\n",
    " -0.11920138  0.02610456 -0.01187746  0.02057748 -0.0062471  -0.02288416\\\n",
    "  0.03060456 -0.14499602'\n",
    "man = man.split()\n",
    "man = map(float, man)\n",
    "man = np.array(man)\n",
    "print(man)\n",
    "\n",
    "\n",
    "queen = '0.10719869 -0.10232529 -0.02304027 -0.02485728  0.17746936 -0.16133316\\\n",
    " -0.09617456 -0.08252326  0.03485161 -0.06539463 -0.05119734 -0.0071219\\\n",
    "  0.04664997  0.07755207 -0.0376812  -0.05126946  0.04974871 -0.09130801\\\n",
    "  0.02200206 -0.11675026  0.05019299  0.03069795  0.04541247 -0.06323202\\\n",
    "  0.00760292  0.00975615  0.06263665 -0.15049373  0.10783202 -0.14120691\\\n",
    "  0.12385137  0.15252414 -0.04066575 -0.01268648 -0.18778183 -0.09001921\\\n",
    "  0.10144432  0.03953693 -0.05206658  0.08898162 -0.06332143 -0.03476854\\\n",
    "  0.03007286  0.15931943  0.00775215  0.03130399  0.05636695 -0.04932799\\\n",
    "  0.00386375 -0.03799838  0.07498509  0.05205622  0.14855604 -0.15902062\\\n",
    "  0.01181984  0.1059057  -0.05600962  0.00615451 -0.12136336  0.05390648\\\n",
    "  0.06225348 -0.05576621  0.06548391  0.117924    0.04091459 -0.09727937\\\n",
    " -0.12719241  0.07945643  0.00403882 -0.08504421  0.01977304  0.01947873\\\n",
    "  0.02801108  0.04144734  0.06505365  0.04846439 -0.03233812  0.0376178\\\n",
    " -0.03111132 -0.14116828  0.04352442  0.03442268  0.09748826 -0.10943903\\\n",
    " -0.02274107 -0.03172036 -0.04356686 -0.02012795  0.14008713 -0.1183772\\\n",
    " -0.08421071 -0.03655564  0.17045356 -0.02569478 -0.15899356 -0.21644557\\\n",
    " -0.01376982 -0.06579022 -0.15950891 -0.20833515 -0.13493483 -0.02202897\\\n",
    " -0.05435896 -0.08009836 -0.05089762 -0.12733521  0.09836871  0.05449677\\\n",
    "  0.10958663 -0.00039016 -0.05642409 -0.14433326  0.0749191   0.07995714\\\n",
    "  0.05160326  0.09664808 -0.01128155 -0.05913845  0.02192012 -0.14660437\\\n",
    "  0.10578209 -0.04522621  0.1281876   0.10627382  0.02222264 -0.15014711\\\n",
    " -0.03498883  0.03361606'\n",
    "queen = queen.split()\n",
    "queen = map(float, queen)\n",
    "queen = np.array(queen)\n",
    "print(queen)\n",
    "from numpy import linalg as LA\n",
    "print(LA.norm(queen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.018674117491851477"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(king, woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
